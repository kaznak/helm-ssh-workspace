name: Helm Chart Test and Validation

on:
  push:
    branches:
      - main
      - develop
    paths:
      - 'helm/**'
      - 'docker/**'
      - '.github/workflows/helm-test.yml'
  pull_request:
    branches:
      - main
    paths:
      - 'helm/**'
      - 'docker/**'
      - '.github/workflows/helm-test.yml'
  workflow_dispatch:

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  # Test environment uses local images only
  TEST_IMAGE_NAME: localhost/ssh-workspace
  TEST_IMAGE_TAG: test

jobs:
  prepare:
    name: Prepare Test Environment
    runs-on: ubuntu-latest
    outputs:
      test-image: ${{ env.TEST_IMAGE_NAME }}:${{ env.TEST_IMAGE_TAG }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Generate SSH test key pair
        run: |
          echo "=== Generating SSH test key pair ==="
          ssh-keygen -t ed25519 -f /tmp/test_ssh_key -N "" -C "ci-test@example.com"
          
          # Export keys for use in subsequent steps
          echo "TEST_SSH_PUBLIC_KEY=$(cat /tmp/test_ssh_key.pub)" >> $GITHUB_ENV
          echo "TEST_SSH_PRIVATE_KEY_PATH=/tmp/test_ssh_key" >> $GITHUB_ENV
          
          # Create base64 encoded private key for Helm values
          TEST_SSH_PRIVATE_KEY_B64=$(base64 -w0 /tmp/test_ssh_key)
          echo "TEST_SSH_PRIVATE_KEY_B64=$TEST_SSH_PRIVATE_KEY_B64" >> $GITHUB_ENV
          
          echo "Generated SSH public key: $(cat /tmp/test_ssh_key.pub)"

      - name: Check Dockerfile with hadolint
        uses: hadolint/hadolint-action@v3.1.0
        with:
          dockerfile: docker/Dockerfile
          failure-threshold: warning
          config: .hadolint.yaml

      - name: Install Helm
        uses: azure/setup-helm@v4
        with:
          version: '3.12.1'

      - name: Lint Helm Chart
        run: |
          helm lint helm/ssh-workspace \
            --set user.name="testuser" \
            --set "ssh.publicKeys[0]=$TEST_SSH_PUBLIC_KEY" \
            --set ssh.testKeys.enabled=true \
            --set "ssh.testKeys.keyPairs[0].publicKey=$TEST_SSH_PUBLIC_KEY" \
            --set "ssh.testKeys.keyPairs[0].privateKey=$(cat /tmp/test_ssh_key)" \
            --debug

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build and cache Docker image
        uses: docker/build-push-action@v6
        with:
          context: docker/
          push: false
          tags: ${{ env.TEST_IMAGE_NAME }}:${{ env.TEST_IMAGE_TAG }}
          cache-from: type=gha,scope=ssh-workspace-test
          cache-to: type=gha,mode=max,scope=ssh-workspace-test
          outputs: type=docker,dest=/tmp/ssh-workspace-image.tar

      - name: Upload Docker image artifact
        uses: actions/upload-artifact@v4
        with:
          name: ssh-workspace-image
          path: /tmp/ssh-workspace-image.tar
          retention-days: 1

      - name: Upload SSH test keys artifact
        uses: actions/upload-artifact@v4
        with:
          name: ssh-test-keys
          path: |
            /tmp/test_ssh_key
            /tmp/test_ssh_key.pub
          retention-days: 1

  test:
    name: Test (${{ matrix.config.name }})
    runs-on: ubuntu-latest
    needs: prepare
    strategy:
      matrix:
        # Test explicit permission strategy with different storage backends
        # Testing chmod behavior across various Kubernetes volume types
        config:
          # emptyDir volumes (current behavior - chmod fails)
          - name: "basic-emptydir"
            permission_strategy: "explicit"
            security_level: "basic"
            persistence_enabled: "false"
            use_container_filesystem: "false"
          - name: "std-emptydir"
            permission_strategy: "explicit"
            security_level: "standard" 
            persistence_enabled: "false"
            use_container_filesystem: "false"
          
          # PVC volumes (expected - chmod should work)
          - name: "basic-pvc"
            permission_strategy: "explicit"
            security_level: "basic"
            persistence_enabled: "true"
            use_container_filesystem: "false"
          - name: "std-pvc"
            permission_strategy: "explicit"
            security_level: "standard"
            persistence_enabled: "true"
            use_container_filesystem: "false"
          
          # Container filesystem (no volume mount - expected chmod should work)
          - name: "basic-container"
            permission_strategy: "explicit"
            security_level: "basic"
            persistence_enabled: "false"
            use_container_filesystem: "true"
          - name: "std-container"
            permission_strategy: "explicit"
            security_level: "standard"
            persistence_enabled: "false"
            use_container_filesystem: "true"
      fail-fast: false
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download SSH test keys artifact
        uses: actions/download-artifact@v4
        with:
          name: ssh-test-keys
          path: /tmp

      - name: Setup SSH test keys
        run: |
          echo "=== Setting up SSH test keys from artifact ==="
          chmod 600 /tmp/test_ssh_key
          chmod 644 /tmp/test_ssh_key.pub
          
          # Export keys for use in subsequent steps
          echo "TEST_SSH_PUBLIC_KEY=$(cat /tmp/test_ssh_key.pub)" >> $GITHUB_ENV
          echo "TEST_SSH_PRIVATE_KEY_PATH=/tmp/test_ssh_key" >> $GITHUB_ENV
          
          echo "Using SSH public key: $(cat /tmp/test_ssh_key.pub)"

      - name: Install Helm
        uses: azure/setup-helm@v4
        with:
          version: '3.12.1'

      - name: Download Docker image artifact
        uses: actions/download-artifact@v4
        with:
          name: ssh-workspace-image
          path: /tmp

      - name: Load Docker image from artifact
        run: |
          echo "Loading Docker image from artifact..."
          docker load -i /tmp/ssh-workspace-image.tar
          docker images | grep ssh-workspace

      - name: Create kind cluster
        uses: helm/kind-action@v1.8.0
        with:
          cluster_name: test-cluster-${{ matrix.config.name }}
          kubectl_version: v1.27.3

      - name: Load test image into kind cluster
        run: |
          # Load the test image into kind cluster for Helm chart testing
          kind load docker-image ${{ env.TEST_IMAGE_NAME }}:${{ env.TEST_IMAGE_TAG }} --name test-cluster-${{ matrix.config.name }} -v 3

      - name: Validate Helm Chart Templates with Kubernetes
        run: |
          # Validate Helm templates using the test image we just built
          helm template test helm/ssh-workspace \
            --set image.repository=${{ env.TEST_IMAGE_NAME }} \
            --set image.tag=${{ env.TEST_IMAGE_TAG }} \
            --set image.pullPolicy=Never \
            --set user.name="testuser" \
            --set "ssh.publicKeys[0]=$TEST_SSH_PUBLIC_KEY" \
            --set ssh.testKeys.enabled=true \
            --set "ssh.testKeys.keyPairs[0].publicKey=$TEST_SSH_PUBLIC_KEY" \
            --set "ssh.testKeys.keyPairs[0].privateKey=$(cat /tmp/test_ssh_key)" \
            --set security.permissionStrategy=${{ matrix.config.permission_strategy }} \
            --set security.level=${{ matrix.config.security_level }} \
            --set persistence.enabled=${{ matrix.config.persistence_enabled }} \
            --set test.useContainerFilesystem=${{ matrix.config.use_container_filesystem }} \
            --validate \
            --debug
          echo "✓ Helm template validation with Kubernetes API successful"

      - name: Install Helm Chart
        run: |
          echo "=== Installing Helm Chart: ${{ matrix.config.name }} ==="
          helm install test-${{ matrix.config.name }} helm/ssh-workspace \
            --set image.repository=${{ env.TEST_IMAGE_NAME }} \
            --set image.tag=${{ env.TEST_IMAGE_TAG }} \
            --set image.pullPolicy=Never \
            --set user.name="testuser" \
            --set "ssh.publicKeys[0]=$TEST_SSH_PUBLIC_KEY" \
            --set ssh.testKeys.enabled=true \
            --set "ssh.testKeys.keyPairs[0].publicKey=$TEST_SSH_PUBLIC_KEY" \
            --set "ssh.testKeys.keyPairs[0].privateKey=$(cat /tmp/test_ssh_key)" \
            --set security.permissionStrategy=${{ matrix.config.permission_strategy }} \
            --set security.level=${{ matrix.config.security_level }} \
            --set persistence.enabled=${{ matrix.config.persistence_enabled }} \
            --set test.useContainerFilesystem=${{ matrix.config.use_container_filesystem }} \
            --timeout=300s \
            --wait \
            --debug
          echo "✓ Helm chart installation completed"

      - name: Debug Deployment Configuration
        run: |
          echo "=== Checking Deployment Readiness Probe Configuration ==="
          kubectl get deployment test-${{ matrix.config.name }}-ssh-workspace -o yaml | grep -A 10 -B 5 readinessProbe || echo "No readiness probe found"
          
          echo "=== Checking Pod Status ==="
          kubectl get pods -l app.kubernetes.io/instance=test-${{ matrix.config.name }}
          
          echo "=== Checking Pod Readiness Details ==="
          kubectl describe pod -l app.kubernetes.io/instance=test-${{ matrix.config.name }} | grep -A 10 -B 5 "Readiness\|Ready\|port"

      - name: Run Helm Tests
        run: |
          helm test test-${{ matrix.config.name }} --timeout=300s

      - name: Debug SSH Authentication Files
        run: |
          echo "=== SSH Authentication Debugging ===" 
          echo "Configuration: ${{ matrix.config.name }}"
          
          # Get pod name
          echo "Finding pods with labels:"
          kubectl get pods -l app.kubernetes.io/name=ssh-workspace,app.kubernetes.io/instance=test-${{ matrix.config.name }}
          
          # Get deployment pod (not test pods) - test pods have helm.sh/hook annotation
          POD_NAME=$(kubectl get pods -l app.kubernetes.io/name=ssh-workspace,app.kubernetes.io/instance=test-${{ matrix.config.name }} --field-selector=status.phase=Running -o jsonpath='{.items[?(@.metadata.annotations.helm\.sh/hook=="")].metadata.name}' || true)
          
          # Fallback: try without field selector and filter test pods by annotation
          if [ -z "$POD_NAME" ]; then
            POD_NAME=$(kubectl get pods -l app.kubernetes.io/name=ssh-workspace,app.kubernetes.io/instance=test-${{ matrix.config.name }} -o jsonpath='{.items[?(@.metadata.annotations.helm\.sh/hook=="")].metadata.name}' || true)
          fi
          
          # Final fallback: get first pod and filter manually
          if [ -z "$POD_NAME" ]; then
            ALL_PODS=$(kubectl get pods -l app.kubernetes.io/name=ssh-workspace,app.kubernetes.io/instance=test-${{ matrix.config.name }} -o jsonpath='{.items[*].metadata.name}')
            for pod in $ALL_PODS; do
              HOOK_ANNOTATION=$(kubectl get pod "$pod" -o jsonpath='{.metadata.annotations.helm\.sh/hook}' 2>/dev/null || true)
              if [ -z "$HOOK_ANNOTATION" ]; then
                POD_NAME="$pod"
                break
              fi
            done
          fi
          
          echo "Debugging pod: $POD_NAME"
          
          if [ -z "$POD_NAME" ]; then
            echo "❌ ERROR: Could not find workspace pod"
            kubectl get pods -l app.kubernetes.io/instance=test-${{ matrix.config.name }}
            exit 0  # Continue with test, but skip this debugging step
          fi
          
          # Check SSH file permissions and structure
          echo "=== SSH Files Analysis ==="
          kubectl exec "$POD_NAME" -- /bin/sh -c '
            echo "=== Home directory analysis ==="
            ls -la /home/testuser/
            echo "=== SSH directory analysis ==="
            if [ -d "/home/testuser/.ssh" ]; then
              ls -la /home/testuser/.ssh/
              echo "=== authorized_keys content check ==="
              if [ -f "/home/testuser/.ssh/authorized_keys" ]; then
                echo "authorized_keys file exists"
                wc -l /home/testuser/.ssh/authorized_keys
                echo "First line of authorized_keys:"
                head -1 /home/testuser/.ssh/authorized_keys
              else
                echo "authorized_keys file not found"
              fi
            else
              echo ".ssh directory not found"
            fi
            
            echo "=== SSH daemon configuration check ==="
            echo "SSH daemon process:"
            ps aux | grep sshd | grep -v grep || echo "No sshd process found"
            
            echo "=== SSH host keys check ==="
            ls -la /etc/ssh/ssh_host_*
            
            echo "=== System permission analysis ==="
            echo "Process user info:"
            id
            echo "File ownership analysis:"
            stat -c "Home: %U:%G (%a)" /home/testuser 2>/dev/null || echo "Cannot stat home directory"
            stat -c ".ssh: %U:%G (%a)" /home/testuser/.ssh 2>/dev/null || echo "Cannot stat .ssh directory"  
            stat -c "authorized_keys: %U:%G (%a)" /home/testuser/.ssh/authorized_keys 2>/dev/null || echo "Cannot stat authorized_keys"
          ' || echo "Failed to execute SSH debugging commands"

      - name: SSH Connectivity Test (External)
        run: |
          echo "=== External SSH Connectivity Test ==="
          echo "Configuration: ${{ matrix.config.name }}"
          
          # Wait for pod to be ready
          echo "Waiting for pod to be ready..."
          kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=ssh-workspace --timeout=120s
          
          # Get service information
          SERVICE_PORT=$(kubectl get service test-${{ matrix.config.name }}-ssh-workspace -o jsonpath='{.spec.ports[0].port}')
          echo "Service port: $SERVICE_PORT"
          
          # Port forward for SSH access
          echo "Setting up port forward..."
          kubectl port-forward service/test-${{ matrix.config.name }}-ssh-workspace 2222:$SERVICE_PORT &
          PORT_FORWARD_PID=$!
          sleep 10
          
          # Test external SSH connectivity only
          echo "Testing external SSH connection..."
          ssh -i $TEST_SSH_PRIVATE_KEY_PATH \
              -o StrictHostKeyChecking=no \
              -o UserKnownHostsFile=/dev/null \
              -o ConnectTimeout=10 \
              -o BatchMode=yes \
              testuser@localhost -p 2222 \
              'echo "External SSH connection successful! Configuration: ${{ matrix.config.name }}"' || {
            echo "❌ External SSH connection failed!"
            
            # Basic debugging
            echo "=== Network connectivity test ==="
            nc -zv localhost 2222 || echo "Port 2222 not accessible"
            
            echo "=== Service status ==="
            kubectl get service test-${{ matrix.config.name }}-ssh-workspace -o wide
            
            # Kill port forward and exit
            kill $PORT_FORWARD_PID 2>/dev/null || true
            exit 1
          }
          
          # Cleanup
          kill $PORT_FORWARD_PID 2>/dev/null || true
          
          echo "✓ External SSH connectivity test completed successfully!"

      - name: Check deployment status
        if: always()
        run: |
          echo "=== Final Helm Status ==="
          helm status test-${{ matrix.config.name }} || echo "Helm release not found"
          
          echo "=== Getting all resources ==="
          kubectl get all -l app.kubernetes.io/instance=test-${{ matrix.config.name }} -o wide
          
          echo "=== Describing pods ==="
          kubectl describe pod -l app.kubernetes.io/instance=test-${{ matrix.config.name }}
          
          echo "=== Pod Logs ==="
          for pod in $(kubectl get pods -l app.kubernetes.io/instance=test-${{ matrix.config.name }} -o name 2>/dev/null); do
            echo "[POD] $pod"
            
            echo "[INIT-CONTAINER:ssh-setup]"
            kubectl logs $pod -c ssh-setup --tail=30 2>/dev/null || echo "[NO-LOGS]"
            
            echo "[MAIN-CONTAINER:ssh-workspace]"
            kubectl logs $pod -c ssh-workspace --tail=30 2>/dev/null || echo "[NO-LOGS]"
            
            echo "[PREVIOUS:ssh-workspace]"
            kubectl logs $pod -c ssh-workspace --previous=true --tail=30 2>/dev/null || echo "[NO-PREVIOUS-LOGS]"
          done
          
          echo "=== Test Pod Logs (All Test Pods) ==="
          for test_pod in $(kubectl get pods -l "helm.sh/hook=test" -o name 2>/dev/null); do
            echo "[TEST-POD] $test_pod"
            pod_status=$(kubectl get $test_pod -o jsonpath='{.status.phase}')
            echo "[STATUS] $pod_status"
            
            # Get all containers in the test pod
            containers=$(kubectl get $test_pod -o jsonpath='{.spec.containers[*].name}')
            for container in $containers; do
              echo "[CONTAINER:$container]"
              kubectl logs $test_pod -c $container --tail=100 2>/dev/null || echo "[NO-LOGS-FOR-$container]"
            done
            echo "--- End of $test_pod ---"
          done
          
          echo "=== Failed Test Pod Specific Logs ==="
          # Try to get logs for the specific permission test pod with more details
          PERMISSION_TEST_POD="test-${{ matrix.config.name }}-ssh-workspace-permission-test"
          echo "Attempting to get logs from: $PERMISSION_TEST_POD"
          
          # Check if the pod exists
          if kubectl get pod "$PERMISSION_TEST_POD" >/dev/null 2>&1; then
            echo "Permission test pod exists, getting logs..."
            kubectl logs "$PERMISSION_TEST_POD" -c permission-validation --tail=200 2>/dev/null || {
              echo "Cannot get container logs, checking pod status..."
              kubectl describe pod "$PERMISSION_TEST_POD" | tail -20
            }
          else
            echo "Permission test pod does not exist, checking all test pods..."
            kubectl get pods | grep test || echo "No test pods found"
          fi
          
          if [ -z "$(kubectl get pods -l app.kubernetes.io/instance=test-${{ matrix.config.name }} -o name 2>/dev/null)" ]; then
            echo "[NO-PODS-FOUND]"
          fi
          
          echo "=== All Events ==="
          kubectl get events --sort-by='.lastTimestamp' | grep test-${{ matrix.config.name }} || echo "No test-${{ matrix.config.name }} events found"
          
          echo "=== All Recent Events ==="
          kubectl get events --sort-by='.lastTimestamp' | tail -20
          
          echo "=== Deployment troubleshooting ==="
          kubectl get deployment test-${{ matrix.config.name }}-ssh-workspace -o yaml || echo "Deployment not found"
          
          echo "=== ReplicaSet troubleshooting ==="
          kubectl get rs -l app.kubernetes.io/instance=test-${{ matrix.config.name }} -o yaml || echo "ReplicaSet not found"

      - name: Cleanup
        if: always()
        run: |
          echo "=== Cleaning up ==="
          # Clean up SSH test keys
          rm -f $TEST_SSH_PRIVATE_KEY_PATH $TEST_SSH_PRIVATE_KEY_PATH.pub 2>/dev/null || true
          
          helm uninstall test-${{ matrix.config.name }} --debug || true
          echo "=== Final cluster state ==="
          kubectl get all -A | grep test-${{ matrix.config.name }} || echo "No remaining test-${{ matrix.config.name }} resources"

  cleanup:
    name: Cleanup Artifacts
    runs-on: ubuntu-latest
    needs: [prepare, test]
    if: always()
    steps:
      - name: List artifacts before cleanup
        run: |
          echo "=== Cleaning up workflow artifacts ==="
          echo "The following artifacts will be deleted to save storage:"
          echo "  - ssh-workspace-image (Docker image)"
          echo "  - ssh-test-keys (SSH key pair)"

      - name: Delete Docker image artifact
        uses: geekyeggo/delete-artifact@v5
        with:
          name: ssh-workspace-image
          failOnError: false

      - name: Delete SSH test keys artifact
        uses: geekyeggo/delete-artifact@v5
        with:
          name: ssh-test-keys
          failOnError: false

      - name: Cleanup completion
        run: |
          echo "✅ Artifact cleanup completed successfully"
          echo "Storage space has been freed for future workflow runs"

